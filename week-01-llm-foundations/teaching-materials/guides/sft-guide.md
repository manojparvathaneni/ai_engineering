# Supervised Fine-Tuning (SFT) Guide

## The Goal

Transform a **completion model** into an **instruction-following model**.

---

## The Analogy: New Employee Training

**Pre-training** = Reading every book in the library. You know a lot, but don't know how to apply it at work.

**SFT** = Your first week at a new job. Someone shows you example emails, example reports, example customer responses. You learn the format and style expected here.

After SFT, you can write professional emails - but you can't yet judge which of two drafts your boss would prefer. That comes later (RLHF).

---

## The Transformation

### Before SFT (Base Model)

```
User: I want to learn ML. What should I do?

Model: I want to learn ML. What should I do if I don't have a 
       math background? Many people ask this question when 
       starting their journey into artificial intelligence...
```

The model **continues the text pattern**. It's not wrong - it's doing exactly what it was trained to do.

### After SFT

```
User: I want to learn ML. What should I do?

Model: Great question! I'd recommend starting with Andrew Ng's 
       Machine Learning course on Coursera. It's beginner-friendly 
       and covers the fundamentals. After that, you could move on 
       to hands-on projects using scikit-learn or PyTorch.
```

The model **answers the question**. It learned a new pattern: "instruction → helpful response."

---

## Two Steps: Data Preparation + Training

```
┌─────────────────────────────────────────────────────────────┐
│  Step 1: Data Preparation                                   │
│  Create/collect demonstration data (prompt/response pairs)  │
└─────────────────────────────────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────────┐
│  Step 2: Training                                           │
│  Same as pre-training, but only learn from responses        │
└─────────────────────────────────────────────────────────────┘
```

---

## Step 1: Data Preparation

### What is Demonstration Data?

Examples that **demonstrate** ideal assistant behavior.

**Simple format (instruction/output):**
```json
{
  "instruction": "Explain photosynthesis to a 5-year-old",
  "output": "Plants are like little chefs that make their own food! They use sunlight as their oven, water as one ingredient, and air as another. When they mix these together with the green stuff in their leaves, they make sugar to eat!"
}
```

**Conversation format (multi-turn):**
```json
{
  "messages": [
    {"role": "user", "content": "What's the capital of France?"},
    {"role": "assistant", "content": "The capital of France is Paris."},
    {"role": "user", "content": "What's it famous for?"},
    {"role": "assistant", "content": "Paris is famous for the Eiffel Tower, world-class art museums like the Louvre, its café culture, and being a global center for fashion and cuisine."}
  ]
}
```

### Pre-training Data vs. Demonstration Data

| Aspect | Pre-training Data | Demonstration Data |
|--------|------------------|-------------------|
| **Format** | Raw text | Prompt/Response pairs |
| **Source** | Web crawl, books, code | Expert-curated examples |
| **Scale** | Trillions of tokens | ~10K-100K examples |
| **Quality** | Filtered but variable | High-quality, curated |
| **Purpose** | Learn language patterns | Learn to follow instructions |

### Where Does Demonstration Data Come From?

**Option 1: Human Experts (Highest Quality)**

The OpenAI approach (InstructGPT):
- Hired 40 expert contractors
- Carefully selected for writing ability
- Created ~13,000 demonstrations
- Result: Enough to transform GPT-3 into InstructGPT

**The analogy:** Hiring professional chefs to cook 13,000 example dishes. Expensive, but the quality is impeccable.

**Option 2: Crowdsourcing (Medium Quality, More Scale)**

- OpenAssistant: 161K conversations from volunteers
- Dolly (Databricks): 15K examples from employees

**The analogy:** Getting home cooks to contribute recipes. More variety, but inconsistent quality.

**Option 3: Synthetic/AI-Generated (Scalable)**

- Alpaca: 52K examples generated by GPT-3.5
- Self-Instruct: Model generates its own training data

**The analogy:** Using a recipe generator to create thousands of recipes. Cheap and fast, but may have subtle issues.

### Key Insight: Quality > Quantity

```
13K expert examples (InstructGPT) 
    beats
52K synthetic examples (Alpaca)
    beats  
1M mixed-quality examples
```

A few thousand excellent examples often outperform millions of mediocre ones. Why? The model learns the pattern quickly - what matters is the pattern being correct.

### Open Source Datasets

| Dataset | Size | Source | Notes |
|---------|------|--------|-------|
| **FLAN Collection** | ~1M+ | Google | Converted from NLP tasks |
| **Alpaca** | 52K | Stanford/GPT-3.5 | Synthetic, commercially usable |
| **Dolly** | 15K | Databricks | Employee-written, open license |
| **OpenAssistant** | 161K | Community | Crowdsourced conversations |
| **ShareGPT** | 90K+ | Users | Real ChatGPT conversations |

---

## Step 2: Training

### The Key Insight: Training is (Almost) Identical to Pre-training!

Same forward pass. Same loss function. Same backpropagation. Same optimizer.

```python
# Pre-training loop
for batch in pretraining_data:
    prediction = model(text)
    loss = cross_entropy(prediction, next_token)
    loss.backward()
    optimizer.step()

# SFT loop (identical structure!)
for batch in demonstration_data:
    prediction = model(prompt + response)
    loss = cross_entropy(prediction, next_token)
    loss.backward()
    optimizer.step()
```

### The One Difference: Loss Masking

We only compute loss on the **response** tokens, not the prompt.

```
Input:      [User] [What] [is] [2+2?] [Assistant] [The] [answer] [is] [4]
Loss mask:    0      0     0     0        0         1      1      1    1
                   ignore these                    ↑ learn from these
```

**Why?** We want the model to learn to **generate good responses**, not to **generate user questions**.

```python
def compute_sft_loss(model, prompt, response):
    # Combine prompt and response
    full_sequence = prompt + response
    
    # Forward pass
    logits = model(full_sequence[:-1])  # predict next token
    targets = full_sequence[1:]
    
    # Create mask: 1 for response tokens, 0 for prompt tokens
    mask = [0] * len(prompt) + [1] * len(response)
    mask = mask[:-1]  # align with targets
    
    # Compute loss only on response
    loss = cross_entropy(logits, targets, reduction='none')
    masked_loss = (loss * mask).sum() / sum(mask)
    
    return masked_loss
```

### Hyperparameter Differences

| Parameter | Pre-training | SFT |
|-----------|--------------|-----|
| **Learning rate** | 1e-4 to 6e-4 | 1e-5 to 2e-5 |
| **Epochs** | <1 (never repeat) | 2-5 (can repeat) |
| **Data size** | Trillions of tokens | Thousands of examples |
| **Batch size** | Large (millions of tokens) | Smaller |

**Why the lower learning rate?**

We're **fine-tuning**, not training from scratch. The model already has valuable knowledge from pre-training. Big updates would destroy this knowledge.

**The analogy:** Teaching someone new skills vs. fixing their existing habits. Small adjustments preserve what they already know. Big changes cause "catastrophic forgetting."

### Chat Templates

Different models use different formats to mark conversation turns:

**ChatML (OpenAI style):**
```
<|im_start|>user
What is the capital of France?<|im_end|>
<|im_start|>assistant
The capital of France is Paris.<|im_end|>
```

**Llama/Alpaca style:**
```
[INST] What is the capital of France? [/INST] The capital of France is Paris.
```

**Why it matters:** The model learns these tokens mark turn boundaries. Using the wrong template = confused outputs.

---

## Why SFT Alone Isn't Enough

After SFT, the model follows instructions. But it's **not ready to deploy**.

### Problem 1: Can't Cover Everything

You can't write demonstrations for every possible question. The model needs to generalize.

### Problem 2: Averages Over Styles

Different annotators write differently:
- Some are concise, others verbose
- Some use bullet points, others prose
- Some are formal, others casual

The model learns a blend that might not match user preferences.

### Problem 3: No Preference Signal

Given two reasonable responses, which is **better**?

```
User: Explain quantum computing

Response A: 3 paragraphs, accurate, technical
Response B: 2 paragraphs, accurate, great analogy  
Response C: 5 paragraphs, thorough, overwhelming
```

All three could be in SFT data. SFT can't express "B > A > C".

**The analogy:** SFT shows the new employee example emails. They learn the format. But they can't judge which of two drafts the boss would prefer. That's what RLHF teaches.

---

## Summary

| Aspect | Details |
|--------|---------|
| **Goal** | Completion → Instruction-following |
| **Data** | Demonstration pairs (prompt/response) |
| **Scale** | ~10K-100K high-quality examples |
| **Training** | Same as pre-training + loss masking |
| **Key insight** | Quality > Quantity |
| **Limitation** | Can't express preferences |

**Next step:** RLHF teaches which responses are *better*, not just which are *acceptable*.
